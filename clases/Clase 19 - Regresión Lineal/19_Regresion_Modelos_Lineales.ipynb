{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e1e45e86",
      "metadata": {
        "id": "e1e45e86"
      },
      "source": [
        "# Clase 19 - Regresión Lineal"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55daed40",
      "metadata": {
        "id": "55daed40"
      },
      "source": [
        "## Regresión\n",
        "\n",
        "\n",
        "La regresión es un problema de aprendizaje supervisado que consiste en generar modelos que sean capaces de asignar un valor real (target) a un conjunto de observaciones (atributos).\n",
        "\n",
        "\n",
        "Uno de los ejemplos más clásicos (que incluso usamos al comienzo del curso) es, dados los atributos de una vivienda, construir modelos que permitan asignarle un precio de forma automatizada. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f60028ec",
      "metadata": {
        "id": "f60028ec"
      },
      "source": [
        "\n",
        "## Regresión Lineal\n",
        "\n",
        "Sea un conjunto de ejemplos etiquetados (i.e., vectores de $D$ características con sus respectivos targets)\n",
        "$\\{(\\mathbf{x}^{(i)}, y^{(i)}) \\}_{i=1}^{N}$ respectivamente y con N la cantidad de ejemplos. $x_j^{(i)}$ con $j = 1, \\dots, D$ es el vector de características que describe al ejemplo $i$\n",
        "\n",
        "\n",
        "Una Regresión Lineal consiste en la construcción de un modelo $f_{w,b} = wx + b$ orientado a predecir el target usando una combinación lineal de las características de cada ejemplo de la forma: \n",
        "\n",
        "$$f_{\\mathbf{w}, b}(x) = \\mathbf{wx} + b$$\n",
        "\n",
        "$$f_{\\mathbf{w}, b}(x) = w_{0} x_0 + w_1 x_1 + \\dots + w_n x_n$$\n",
        "\n",
        "En donde $\\mathbf{w}$ es el vector de parámetros de tamaño $D$ (que define una pendiente en un hiperplano) y $b$ el intercepto. \n",
        "En este caso, se dice que el modelo construido $f_{\\mathbf{w}, b}$ está *parametrizado* por $\\mathbf{w}$ y $b$.\n",
        "\n",
        "$f_{\\mathbf{w}, b}(x)$ también se le denomina $\\hat{y_i}$, el cuál es simplemente el valor predicho por el modelo.\n",
        "\n",
        "\n",
        "Cada posible combinación de los parámetros generará una regresión distinta. Por ende, la idea es encontrar el conjunto de parámetros que más se ajusten a los datos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d20b17dd",
      "metadata": {
        "id": "d20b17dd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/maranedah/MDS7202/main/clases/Clase%2019%20-%20Regresi%C3%B3n%20Lineal/resources/auto-mpg.csv')\n",
        "fig = px.scatter(\n",
        "    df,\n",
        "    x='displacement',\n",
        "    y='mpg',\n",
        "    template='plotly_white',\n",
        "    color='mpg',\n",
        "    hover_name='car name',\n",
        "    trendline='ols', # este parámetro permite calcular rápidamente la regresión sobre x e y.\n",
        "    title=\n",
        "    \"Cilindrada de Distintos Automóviles con Respecto a su Rendimiento (mpg)<br>\"\n",
        "    \"<sub>La linea representa una regresión lineal calculada sobre ambas variables.</sub>\"\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5fd11ad",
      "metadata": {
        "id": "c5fd11ad"
      },
      "source": [
        "\n",
        "Para encontrar una buena configuración de parámetros, intentaremos minimizar la siguiente expresión: \n",
        "\n",
        "\n",
        "$$\\text{Mean Squared Error (MSE)} = \\frac{1}{N}\\sum_{i=1\\dots n} (y_i - f_{\\mathbf{w}, b} (x_i))^2 = \\frac{1}{N}\\sum_{i=1\\dots n} (y_i - \\hat{y_i})^2$$\n",
        "\n",
        "\n",
        "\n",
        "En terminos prácticos, MSE es una medida que penaliza que tan mal predice el modelo. Para esto, promedia los cuadrados de la diferencias entre los valores predichos y los valores reales. \n",
        "\n",
        "<div align='center'>\n",
        "<br>\n",
        "<img src='https://i.ibb.co/jRJ0xxh/mse.png' width=600 />\n",
        "<br>\n",
        "    Fuente: <a href='https://vitalflux.com/mean-square-error-r-squared-which-one-to-use/'>Mean Squared Error or R-Squared – Which one to use?</a>.  \n",
        "</div>\n",
        "    \n",
        "\n",
        "\n",
        "En general, las expresiones (como MSE) que buscan maximizar/minimizar valores se les conoce como **función objetivo/función de costo**, mientras que $(f_{\\mathbf{w}, b} (x_i) - y_i)^2$ se le denomina **función de pérdida**, la cual se define como el costo/penalización de predecir mal un ejemplo $x_i$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a250485c",
      "metadata": {
        "id": "a250485c"
      },
      "source": [
        "\n",
        "### Entrenamiento del Modelo\n",
        "\n",
        "El entrenamiento de los modelos basados en funciones de costo consiste en encontrar los parámetros que permitan reducir al máximo el valor que toma dicha función. En el caso de la regresión lineal, es posible encontrar una expresión analítica (i.e., una fórmula) que permite minimizar la función de costo: Minimos cuadrados.\n",
        "En este caso, solo veremos el cálculo para una regresión con solo un atributo.\n",
        "\n",
        "$$\\text{Mean Squared Error (MSE)} = \\frac{1}{n}\\sum_{i=1}^n (y_i - x_i w - b)^2$$\n",
        "\n",
        "\n",
        "\n",
        "La idea principal detrás de este método es calcular la derivada de MSE sobre cada parámetro ($w$ y $b$) y luego igualar a 0. \n",
        "\n",
        "$$\\frac{\\partial  MSE}{\\partial  w} = 0 \\leftrightarrow  -2 \\sum_{i=1}^{n} (y_i - x_i w - b) x_i = 0$$\n",
        "$$\\frac{\\partial  MSE}{\\partial b} = 0 \\leftrightarrow  -2 \\sum_{i=1}^{n} (y_i - x_i w - b) = 0$$\n",
        "\n",
        "\n",
        "Las ecuaciones anteriores nos permiten calcular $w$ y $b$ un sistema de ecuaciones. Luego de bastantes operaciones algebraicas: \n",
        "\n",
        "\n",
        "$$w = \\frac{\\sum_{i=0}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=0}^{n}(x_i - \\bar{x})^2 }$$\n",
        "\n",
        "$$b = \\bar{y} - w\\bar{x}$$\n",
        "\n",
        "Noten que para $w$, la expresión del dividendo es muy similar al cálculo de una correlación entre los atributos y la variable a predecir.\n",
        "Por otra parte, el divisor es muy similar al cálculo de la varianza de $x$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29f71cbc",
      "metadata": {
        "id": "29f71cbc"
      },
      "outputs": [],
      "source": [
        "# cálculo a mano!\n",
        "\n",
        "x = df['displacement']\n",
        "y = df['mpg']\n",
        "\n",
        "w = sum((x - x.mean()) * (y - y.mean())) / sum((x - x.mean())**2)\n",
        "b = y.mean() - w * x.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dffec8e",
      "metadata": {
        "scrolled": false,
        "id": "6dffec8e"
      },
      "outputs": [],
      "source": [
        "print(f'f(x) = {round(w, 3)}x + {round(b, 3)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "426b5d33",
      "metadata": {
        "id": "426b5d33"
      },
      "outputs": [],
      "source": [
        "fig = px.scatter(\n",
        "    df,\n",
        "    x='displacement',\n",
        "    y='mpg',\n",
        "    template='plotly_white',\n",
        "    color='mpg',\n",
        "    hover_name='car name',\n",
        "    trendline='ols', # este parámetro permite calcular rápidamente la regresión sobre x e y.\n",
        "    title=\n",
        "    \"Cilindrada de Distintos Automóviles con Respecto a su Rendimiento (mpg)<br>\"\n",
        "    \"<sub>La linea representa una regresión lineal calculada sobre ambas variables.</sub>\"\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "686c47fa",
      "metadata": {
        "id": "686c47fa"
      },
      "source": [
        "\n",
        "## Métricas de Rendimiento de Regresión\n",
        "\n",
        "\n",
        "### Mean Squared Error \n",
        "\n",
        "Una de las principales opciones es utilizar la misma función de costo con la cuál se entrena una Regresión Lineal. Sin embargo, esta cuenta con métricas similares que intentan cuantificar el error de la misma manera:\n",
        "\n",
        "\n",
        "#### RMSE - Root Mean Squared Error\n",
        "\n",
        "Esta variante consiste en calcular la raíz cuadrada de MSE. La idea principal de esta métrica es que el error sea interpretable ya que queda con la unidad que se está prediciendo. \n",
        "\n",
        " $$RMSE = \\sqrt{\\frac{1}{N}\\sum_{i=1\\dots n} (y_i - \\hat{y_i})^2}$$\n",
        " \n",
        " \n",
        "#### MAE - Mean Absolute Error\n",
        "\n",
        "Esta variante no calcula el error de forma cuadrática, si no que lo hace simplemente con un valor absoluto\n",
        "\n",
        " $$MAE = \\frac{1}{N}\\sum_{i=1\\dots n} |y_i - \\hat{y_i}|$$\n",
        " \n",
        " \n",
        "\n",
        "#### MAE - Median Absolute Error\n",
        "\n",
        "Esta variante, muy similar a la anterior, calcula la mediana en vez de la media de las diferencias.\n",
        "\n",
        " $$MedAE = median(|y_1 - \\hat{y_1}|, \\dots, |y_n - \\hat{y_n}|)$$\n",
        " \n",
        "La particularidad de esta formulación es que, a diferencia de la anterior, es resistente a outliers gracias al su cálculo basado en la media."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35c0d2bb",
      "metadata": {
        "id": "35c0d2bb"
      },
      "source": [
        "### Coeficiente de Determinación R²\n",
        "\n",
        "El Coeficiente de Determinación R² es un puntaje que representa la proporción de varianza de los valores predichos por el modelo con respecto a la los target reales.\n",
        "\n",
        "\n",
        "$$R^2 (y, \\hat{y}) = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y_i})^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}$$\n",
        "\n",
        "En donde $y_i$ es el valor real del target de un ejemplo $x_i$, $\\hat{y_i}$ es el valor predicho por el modelo y $\\bar{y}$ es la media de los targets reales ($\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n} y_i$).\n",
        "$\\sum_{i=1}^{n} (y_i - \\hat{y_i})^2$ también se conoce como suma de cuadrática de los residuos.\n",
        "\n",
        "\n",
        "<div align='center'>\n",
        "<br>\n",
        "<img src='https://i.ibb.co/w7WkR0Q/r2.png' width=600 />\n",
        "<br>\n",
        "<div> \n",
        "    Coeficiente de Determinación en <a href='https://en.wikipedia.org/wiki/Coefficient_of_determination'>Wikipedia (Inglés)</a>.\n",
        "    <br>\n",
        "En esta infografía, se explica el R² como 1 menos la suma del area los cuadrados azules dividida por la suma de los cuadrados rojos.\n",
        "    \n",
        "<br>    \n",
        "</div>\n",
        "    \n",
        "<br>\n",
        "\n",
        "Puede ser útil pensar que lo que se está comparando es un modelo baseline que siempre predice la media (el de la izquierda) con respecto a un modelo entrenado. Luego, la proporción indica que tanto mejora el modelo entrenado con respecto al baseline. \n",
        "\n",
        "\n",
        "El mejor puntaje posible es 1 (los mejores están cercanos a este) y este puede ser negativo (con modelos extremadamente malos).\n",
        "    \n",
        "    \n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "888256f0",
      "metadata": {
        "id": "888256f0"
      },
      "source": [
        "### Auto-mpg dataset\n",
        "\n",
        "Para ejemplicar una Regresión Lineal, usaremos el dataset [Auto-mpg dataset](https://www.kaggle.com/uciml/autompg-dataset), el cual, dado distintas características de autos antiguos, intenta predecir el consumo de galones por milla (miles per gallon, mpg)\n",
        "\n",
        "![Auto-mpg dataset](https://storage.googleapis.com/kaggle-datasets-images/1489/2667/d7895dcd2db5e0cfda19c3edc2f2d410/dataset-cover.jpg)\n",
        "\n",
        "<div align='center'>\n",
        "Fuente: Competencia en Kaggle.\n",
        "</div>\n",
        "\n",
        "\n",
        "Son 398 autos. Los atributos que los describen son:\n",
        "\n",
        "    - mpg: continuous\n",
        "    - cylinders: multi-valued discrete\n",
        "    - displacement: continuous\n",
        "    - horsepower: continuous\n",
        "    - weight: continuous\n",
        "    - acceleration: continuous\n",
        "    - model year: multi-valued discrete\n",
        "    - origin: multi-valued discrete\n",
        "    - car name: string (unique for each instance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f093c684",
      "metadata": {
        "id": "f093c684"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/maranedah/MDS7202/main/clases/Clase%2019%20-%20Regresi%C3%B3n%20Lineal/resources/auto-mpg.csv')\n",
        "df = df.dropna()\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb5340fb",
      "metadata": {
        "id": "bb5340fb"
      },
      "outputs": [],
      "source": [
        "px.scatter_matrix(df.drop(columns=['car name', 'origin']),\n",
        "                  title='Scatter Matrix mpg Dataset',\n",
        "                  height=800,\n",
        "                  template='plotly_white',\n",
        "                  color='mpg',\n",
        "                  color_continuous_scale='Viridis',\n",
        "                  hover_name='mpg',\n",
        "                 ).update_traces(diagonal_visible=False, showupperhalf=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04409899",
      "metadata": {
        "id": "04409899"
      },
      "outputs": [],
      "source": [
        "features = df.drop(columns=['car name', 'mpg'])\n",
        "target = df['mpg']\n",
        "\n",
        "features.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb612fb7",
      "metadata": {
        "id": "bb612fb7"
      },
      "source": [
        "En el caso de usar todos los atributos, cada observación $\\mathbf{x_i}$ estará compuesta por `'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model year', 'origin'` y etiquetada con el target a predecir `mpg` (millas por galón). Al igual que en la clasificación, para entrenar el modelo se separa el dataset en entrenamiento y prueba.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cdd6892",
      "metadata": {
        "id": "1cdd6892"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test  = train_test_split(\n",
        "    features, target, shuffle=True, train_size=0.3, random_state=33\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1de89e0e",
      "metadata": {
        "scrolled": true,
        "id": "1de89e0e"
      },
      "outputs": [],
      "source": [
        "X_train.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84c5c4f5",
      "metadata": {
        "id": "84c5c4f5"
      },
      "outputs": [],
      "source": [
        "y_train.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4190c7c",
      "metadata": {
        "id": "d4190c7c"
      },
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
        "\n",
        "preprocessing = ColumnTransformer(\n",
        "    [\n",
        "        (\"standard\", StandardScaler(), [\"displacement\", \"weight\", \"acceleration\"]),\n",
        "        (\"ohe\", OneHotEncoder(drop='first'), [\"origin\"]),\n",
        "        (\"ordinal\", OrdinalEncoder(), [\"cylinders\", \"model year\"]),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ad14add",
      "metadata": {
        "id": "7ad14add"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "pipe = Pipeline([('preprocesamiento', preprocessing),\n",
        "                 ('regresor', LinearRegression())])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ff0f182",
      "metadata": {
        "id": "3ff0f182"
      },
      "outputs": [],
      "source": [
        "pipe.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0bef3f1",
      "metadata": {
        "id": "c0bef3f1"
      },
      "outputs": [],
      "source": [
        "y_pred = pipe.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "500b576a",
      "metadata": {
        "id": "500b576a"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, median_absolute_error\n",
        "\n",
        "def evaluate(y_test, y_pred):\n",
        "\n",
        "    print('MSE:', mean_squared_error(y_test, y_pred), '\\n')\n",
        "    print('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\n",
        "    print('MAE:', mean_absolute_error(y_test, y_pred))\n",
        "    print('MedAE:', median_absolute_error(y_test, y_pred), '\\n')\n",
        "    print('R²:', r2_score(y_test, y_pred))\n",
        "    \n",
        "    \n",
        "evaluate(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f8c7701",
      "metadata": {
        "scrolled": true,
        "id": "8f8c7701"
      },
      "outputs": [],
      "source": [
        "print('Coeficientes de la Regresión (w_i) por Atributo:\\n\\n', pipe[-1].coef_.round(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acf603ae",
      "metadata": {
        "id": "acf603ae"
      },
      "outputs": [],
      "source": [
        "pipe[-1].intercept_ "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac814dd8",
      "metadata": {
        "id": "ac814dd8"
      },
      "source": [
        "### Regularización\n",
        "\n",
        "Regularización es un conjunto de técnicas que fuerzan a la regresión a generar modelos no tan complejos y así evitar el overfitting.\n",
        "\n",
        "<div align='center'>\n",
        "<img src='https://i.ibb.co/kHySW6g/tipos-fit.png' width=800/>\n",
        "</div>\n",
        "\n",
        "La idea por detrás es simple: para crear modelos regularizados se agregan distintas penalizaciones a la función objetivo (MSE). Estas penalizaciones aumentan el valor de MSE a medida que el modelo se hace más complejo (i.e., a medida que los parámetros $w_i$ se hacen más grandes).\n",
        "\n",
        "Las penalizaciones más comunes consisten en agregar a MSE el cálculo de una norma\n",
        "\n",
        "- **L2** sobre los parámetros, también conocida como *Rigde*,  \n",
        "- **L1** sobre los parámetros, también conocida como *Lasso*\n",
        "- **Elastic-Net**, la cuál es una combinación de las anteriores.\n",
        "\n",
        "\n",
        "La idea fundamental de esto es que ningún parámetro sea mucho más grande que el resto. Al penalizar los parámetros grandes, este tipo de técnicas forzará que todos los $w_i$ sean los más cercanos a 0.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "565d8fb3",
      "metadata": {
        "id": "565d8fb3"
      },
      "source": [
        "#### Ridge \n",
        "\n",
        "Como dijimos anteriormente, Ridge agrega una penalización L2 sobre la función objetivo:\n",
        "\n",
        "$$MSE = \\frac{1}{N}\\sum_{i=1\\dots n} (y_i - f_{\\mathbf{w}, b} (x_i))^2  + \\alpha ||w||^2$$ \n",
        "\n",
        "en donde $||w||$ es la norma se define como $||w||^2 = \\sum_{i=1}^D (w_i)^2$ y $\\alpha$ es un hiperparámetro que controla la importancia de la regularización. \n",
        "\n",
        "Mientras menor sea $\\alpha$, menor efecto tendrá la regularización. Por otra parte, mientras mayor sea, es más posible que la regresión no sea capaz de aprender lo suficientemente bien, lo que puede llevar a un *underfitting*\n",
        "\n",
        "L2 en general tiende a dar mejores resultados que una regresión normal cuando hay muchos atributos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13308c23",
      "metadata": {
        "id": "13308c23"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "rigde_pipe = Pipeline([('preprocesamiento', preprocessing) , \n",
        "                       ('regresor', Ridge())])\n",
        "rigde_pipe.fit(X_train, y_train)\n",
        "\n",
        "ridge_y_pred = rigde_pipe.predict(X_test)\n",
        "\n",
        "evaluate(y_test, ridge_y_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5397f57",
      "metadata": {
        "id": "e5397f57"
      },
      "source": [
        "#### Lasso\n",
        "\n",
        "Por otra parte, Lasso agrega una penalización L1 sobre la función objetivo:\n",
        "\n",
        "\n",
        "$$MSE = \\frac{1}{N}\\sum_{i=1\\dots n} (y_i - f_{\\mathbf{w}, b} (x_i))^2  + \\alpha |w|$$ \n",
        "\n",
        "en donde $|w|$ es la suma de los valores absolutos de los parámetros y se define como $|w| = \\sum_{i=1}^D |w_i|$ y $\\alpha$ es un hiperparámetro que controla la importancia de la regularización. \n",
        "\n",
        "Al igual que el caso anterior, mientras menor sea $\\alpha$, menor efecto tendrá la regularización. Por otra parte, mientras mayor sea, es más posible que la regresión no sea capaz de aprender lo suficientemente bien, lo que puede llevar a un *underfitting*\n",
        "\n",
        "Lasso produce modelos sparse, es decir, modelos que muchos de las pendientes de los atributos es igual a 0.\n",
        "En términos prácticos, produce una selección de atributos al conservar solo los atributos más relevantes para predecir.\n",
        "Esto permite tener interpretabilidad del modelo al saber cuales son las variables conservadas y cuales son las descartadas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "043fde65",
      "metadata": {
        "scrolled": true,
        "id": "043fde65"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "lasso_pipe = Pipeline([('preprocesamiento', preprocessing) , \n",
        "                       ('regresor', Lasso(alpha=0.1))])\n",
        "\n",
        "lasso_pipe.fit(X_train, y_train)\n",
        "\n",
        "lasso_y_pred = lasso_pipe.predict(X_test)\n",
        "evaluate(y_test, lasso_y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77597265",
      "metadata": {
        "id": "77597265"
      },
      "outputs": [],
      "source": [
        "print('Coeficientes de la regresión (w_i) usando Lasso por Atributo:\\n\\n', lasso_pipe[-1].coef_.round(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.youtube.com/watch?v=Xm2C_gTAl8c"
      ],
      "metadata": {
        "id": "6T5Pud0sZC22"
      },
      "id": "6T5Pud0sZC22"
    },
    {
      "cell_type": "markdown",
      "id": "094c24a3",
      "metadata": {
        "id": "094c24a3"
      },
      "source": [
        "#### Elastic-Net\n",
        "\n",
        "Por último, Elastic-Net combina ambas penalizaciones utilizando un coeficiente $\\rho$ que las pondera.\n",
        "\n",
        "\n",
        "$$MSE = \\frac{1}{N}\\sum_{i=1\\dots n} (y_i - f_{\\mathbf{w}, b} (x_i))^2  + \\alpha \\rho |w| + (1-\\rho) \\alpha ||w||_2$$ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88a1cf15",
      "metadata": {
        "id": "88a1cf15"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import ElasticNet\n",
        "\n",
        "en_pipe = Pipeline([('preprocesamiento', preprocessing) ,\n",
        "                    ('regresor', ElasticNet(alpha=0.01))])\n",
        "en_pipe.fit(X_train, y_train)\n",
        "\n",
        "en_y_pred = en_pipe.predict(X_test)\n",
        "\n",
        "evaluate(y_test, en_y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c58b0b4d",
      "metadata": {
        "id": "c58b0b4d"
      },
      "source": [
        "### Otros Modelos\n",
        "\n",
        "\n",
        "Scikit-learn implementa una gran variedad de [modelos lineales](https://scikit-learn.org/stable/modules/linear_model.html#multi-task-elastic-net) que mejoran la regresión lineal simple. Entre estos podemos encontrar:\n",
        "\n",
        "- [Bayesian Regression](https://scikit-learn.org/stable/modules/linear_model.html#bayesian-regression) por ejemplo, el cual en el entrenamiento del modelo se estiman los parámetros de la regularización L2.\n",
        "- [Logistic Regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression) la cual adapta una regresión para poder clasificar, todo a través de adaptar la regresión para calcular una probabilidad de que un ejemplo pertenezca a cierta clase (usando la función sigmoidal).\n",
        "- Etc..."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}