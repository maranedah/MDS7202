{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGh-Be9IF3EM"
      },
      "source": [
        "# Clase 17: Introducción al Aprendizaje Supervisado\n",
        "\n",
        "\n",
        "**MDS7202: Laboratorio de Programación Científica para Ciencia de Datos**\n",
        "\n",
        "**Profesor: Matías Rojas**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KocntejMF3EO"
      },
      "source": [
        "## Objetivos de la clase: \n",
        "   \n",
        "- Introducir al aprendizaje supervisado mediante el uso de ejemplos aplicados.\n",
        "- Entender el framework utilizado para resolver la tarea de clasificación y reforzar los contenidos asociados a la ingeniería de características.\n",
        "- Estudiar las métricas de evaluación más comunes para la tarea seleccionada.\n",
        "- Entender la idea de Holdout y K-Fold para evaluar modelos.\n",
        "- Experimentar con los primeros modelos de clasificación del curso.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fODAnNtoF3EP"
      },
      "source": [
        "## Panorama General Hasta el Momento\n",
        "\n",
        "<div align='center'>\n",
        "<img src='https://i.ibb.co/DRvgXs6/etapas.png'/>\n",
        "</div>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "<div align='center'>\n",
        "<img src=\"https://i.ibb.co/BT3Dt2L/machine-learning.png\" alt=\"Panorama General ML: Clasificación supervisada, No supervisada y Aprendizaje Reforzado binario\" width=700/>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1jMgmQ7F3EP"
      },
      "source": [
        "El aprendizaje automático es el estudio de algoritmos que automáticamente mejoran su rendimiento a través de la experiencia. Estos algoritmos construyen modelos basados en datos de muestra con la intención de realizar predicciones sin ser explícitamente programados para hacerlo.\n",
        "\n",
        "## Aprendizaje Supervisado\n",
        "\n",
        "El aprendizaje supervisado se basa en trabajar con datasets cuyas observaciones son **características** que describen a algún objeto. Estas observaciones además cuentan con una **etiqueta/valor real**, la cuál corresponde a una clase o valor que se le asigna a cada observación.\n",
        "\n",
        "Cuando el valor a predecir es un(a): \n",
        "\n",
        "- Categoría/Etiqueta, el problema que se resuelve se denomina **Clasificación**.\n",
        "- Valor real, el problema que se resuelve se denomina **Regresión**.\n",
        "\n",
        "\n",
        "En otras palabras, las etiquetas pertenecen a un número finito de clases. Por ejemplo, en caso que estemos describiendo a una persona, el vector asociado a cada observación puede contener los siguentes **features/características**:\n",
        "\n",
        "- su altura en cm, \n",
        "- edad, \n",
        "- peso en kg, \n",
        "- residencia, \n",
        "- etc...\n",
        "\n",
        "Mientras que la **etiqueta** puede ser si la persona *quiere o no contratar un servicio de internet*, es decir un valor boolenao, $\\{ True, False\\}$ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7SEeIU1F3EP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Ejemplo de un dataset con una etiqueta con un conjunto de datos discreto (clasificación).\n",
        "pd.DataFrame(\n",
        "    [[177, 43, 72, \"Maipú\", True], [160, 16, 60, \"Pudahuel\", False]],\n",
        "    columns=[\"Altura\", \"Edad\", \"Peso\", \"Residencia\", \"Posible cliente?\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "facFLZjbF3EQ"
      },
      "source": [
        "Como también lo que está dispuesto a gastar en el plan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WurUHverF3ER"
      },
      "outputs": [],
      "source": [
        "# Ejemplo de un dataset con una etiqueta con un conjunto de datos continuo (regresión).\n",
        "pd.DataFrame(\n",
        "    [[177, 43, 72, \"Maipú\", 55000], [160, 16, 60, \"Pudahuel\", 0]],\n",
        "    columns=[\"Altura\", \"Edad\", \"Peso\", \"Residencia\", \"Cuánto está dispuesto a pagar?\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35vUe3ZBF3ER"
      },
      "source": [
        "Dado un conjunto de datos etiquetados, el objetivo del aprendizaje supervisado es crear algoritmos/modelos que permitan **asignar de forma automática categorías o valores a observaciones nuevas**. \n",
        "\n",
        "En términos prácticos, dada una nueva observación representada por su vector de características, el modelo generado debe ser capaz de asignar una etiqueta a dicha observación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbkJEaVwF3ER"
      },
      "source": [
        "### Framework General de Aprendizaje Supervisado Clásico\n",
        "\n",
        "La siguiente lista muestra las etapas que debería cumplir un algoritmo de aprendizaje supervisado clásico (i.e., no red neuronal)\n",
        "\n",
        "1. **Feature Engineering y Preprocesamiento**: Recolectar y preparar los datos.\n",
        "2. **Entrenar** un algoritmo de clasificación/regresión usando los datos.\n",
        "3. **Evaluar** qué tan bien el clasificador puede clasificar nuevos ejemplos.\n",
        "4. **Optimizar los modelos** modificando sus hiperparámetros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoYxBzzXF3ER"
      },
      "source": [
        "### Feature Engineering\n",
        "\n",
        "Feature Engineering/Ingeniería de Características es la etapa en que se transforman los \"datos raw/crudos\" de entrada en un dataset, de manera que se tengan datos robustos para ser usados por el clasificador/regresor que desean entrenar.\n",
        "\n",
        "\n",
        "Este proceso incluye: \n",
        "\n",
        "- **Creación de nuevas Features** a partir de operaciones usando los datos disponibles.\n",
        "- **Transformaciones** como las vistas en la clase de preprocesamiento (escalamiento, normalización, one hot encoding para variables categóricas etc...).\n",
        "- **Reducción de Características** en la que se combinan/reducen características redundantes (usando por ejemplo, PCA).\n",
        "- **Selección de Características** en la que a partir de diversos criterios se seleccionan las características que más aportan al modelo.\n",
        "\n",
        "El proceso de generar y preprocesar las features requiere mucha creatividad y al mismo conocimiento del dominio del problema.\n",
        "\n",
        "La creación de nuevas características puede incluir: \n",
        "\n",
        "- **Generación de features a partir de la combinación de otras**: Sumar, restar, multiplicar y contar distintas features puede agregar más información que ellas por si mismas. Esto también incluye el preprocesar features independientemente con funciones no linales como $\\log$, $\\exp$, etc...\n",
        "- **Discretización de una variable numérica a través de Binning**: Transformar una variable numérica a una variable categórica según rangos usando bins o percentiles. Recordar el uso de los métodos `cut` y `qcut`. Ver también `Binarizer` y `KBinsDiscretizer` en `sklearn.preprocessing`.\n",
        "- **Clusters** generados a partir de las features (no incluir las labels! es la información que quieren predecir)\n",
        "- **Bag-of-words para texto**. Técnica similar a One Hot encoding en donde cada palabra es una columna y se cuenta la cantidad de apariciones de cada palabra en una oración.\n",
        "- **Seno/Coseno** para codificar variables cíclicas, como las horas, días, meses o años. Por ejemplo, para los días de la semana en donde $p \\in \\{1,2,3,4,5,6,7\\}$, se pueden generar dos features usando \n",
        "$$\n",
        "p_{sin} = \\sin{\\frac{2 \\times \\pi \\times p}{\\max{p}}} \n",
        "$$\n",
        "y\n",
        "$$\n",
        "p_{cos} = \\cos{\\frac{2 \\times \\pi \\times p}{\\max{p}}} \n",
        "$$\n",
        "- **Datos Temporales**: La idea aquí es transformar toda la secuencia temporal a un vector que la describa. Para esto, se pueden calcular descriptores como media, moda, tiempo entre valles, picos, diferencias entre valles y picos en un determinado tiempo, etc...  .Una librería útil para esto es tsfel: https://tsfel.readthedocs.io/en/latest/\n",
        "- **Transformaciones polinomiales a variables numéricas**: Esto se basa en que en dimensiones más altas/no lineales los datos pueden mostrar patrones que no se presentan en los datos originales y que pueden ser aprendidas por el algoritmo. Ver la transformación `PolynomialFeatures` de `sklearn.preprocessing`.\n",
        "\n",
        "Una feature buena cumple que: \n",
        "\n",
        "- **Tiene un alto poder predictivo**\n",
        "- **Computabilidad rápida** \n",
        "- **No correlación con otras features**\n",
        "\n",
        "Este proceso debe ser determinista ya que al momento de predecir datos nuevos, las transformaciones y features calculadas sobre estos deben ser las mismas que las utilizadas en el proceso de entrenamiento. Para solucionar esto, es muy recomendable usar los `Pipelines`.\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI3z4hNqF3ES"
      },
      "source": [
        "\n",
        "\n",
        "El modelo construido debe **generalizar**, es decir, debe ser capaz de realizar predicciones correctas en nuevas observaciones. Para esto es útil pensar que el modelo generado está separando los datos por clases a través de un *decision boundary*. Mientras más holgado sea este *decision boundary*, mejor podrá generalizar el modelo.\n",
        "\n",
        "<div align='center'>\n",
        "<img src='https://i.ibb.co/3mcx35c/overfitting.png' witdh=400/>\n",
        "\n",
        "</div>\n",
        "\n",
        "<div align='center'>\n",
        "<a href='https://www.researchgate.net/figure/Example-of-overfitting-in-classification-a-Decision-boundary-that-best-fits-training_fig1_349186066'>Ejemplo de *Overfitting* en researchgate</a>\n",
        "</div>\n",
        "    \n",
        "<br/>\n",
        " \n",
        "### Cómo determinar que algorimo utilizar \n",
        " \n",
        "Muchas veces se piensa que lo más importante es la **capacidad predictiva** del modelo.\n",
        "Sin embargo, también hay otros factores muy relevantes que determinarán que algoritmo predictivo utilizar: \n",
        "\n",
        "**Eficiencia**: \n",
        "  - ¿Qué tanto se está demorando mi modelo en entrenar? \n",
        "  - ¿Y en predecir? \n",
        "  - ¿Es eficiente en memoria? \n",
        "  - ¿Debe almacenar el dataset de entrenamiento para funcionar?\n",
        "  - ¿Es posible usarlo en tiempo real para algún tipo de solución online?\n",
        "  \n",
        "**Número de Features y Ejemplos Requeridos**: \n",
        "  - ¿Cuántos datos o features son requeridos para entrenar el modelo?\n",
        "  - ¿Es compatible con la cantidad que dispongo?\n",
        "  - ¿El tipo de features (i.e., categorícas, numéricas, combinación de ambas, etc...) es compatible con el algoritmo?\n",
        "  \n",
        "**Explicabilidad**: \n",
        "  - ¿Puedo explicar por qué el modelo está clasificando/regresionando de la manera que lo hace? \n",
        "  \n",
        "***Fairness***: \n",
        "  - ¿Mi modelo es injusto con respecto a algún grupo social?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7u9VS5RF3ES"
      },
      "source": [
        "\n",
        "### ¿Cómo saber si un modelo es bueno o no?\n",
        "\n",
        "Resumimos la capacidad predictiva de un modelo mediante **métricas de desempeño** (performance metrics).\n",
        "\n",
        "Las métricas se calculan contrastando los valores predichos versus los valores reales de la variable objetivo (con datos no usados durante entrenamiento)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMqpMT3sF3ES"
      },
      "source": [
        "##  Matriz de Confusión"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qFmeiCJF3ES"
      },
      "source": [
        "<div align='center'>\n",
        "    <img src=\"https://i.ibb.co/5sMqPDR/matriz-conf.png\" alt=\"Ejemplo de una matriz de confusión para un problema de clasificación binario\" width=450/>\n",
        "</div>\n",
        "\n",
        "<center>Ejemplo de una matriz de confusión para un problema de clasificación binario.</center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_S3BzvHMF3ES"
      },
      "source": [
        "---\n",
        "\n",
        "> Ejemplo: Alergia a cierto medicamento en donde la clase `+` indica alergia.\n",
        "\n",
        "\n",
        "Nuestro dataset tiene 10.000 observaciones distribuidos de la siguiente forma:\n",
        "\n",
        "- Clase `+`: 100 observaciones.\n",
        "- Clase `-`: 9900 observaciones.\n",
        "\n",
        "\n",
        "Luego, creamos un modelo que clasificó nuestro dataset y graficamos sus resultados a través de la siguiente matriz de confusión:\n",
        "\n",
        "\n",
        "|                    | **Predicha (`+`)**  | **Predicha (`-`)** |\n",
        "|--------------------|---------------------|--------------------|\n",
        "| **Real (`+`)**     | 10                  | 90                 |\n",
        "| **Real (`-`)**     | 100                 | 9800               |\n",
        "\n",
        "---\n",
        "\n",
        "> **Pregunta**: ¿Cuales métricas de desempeño conocen para evaluar este caso?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJoJcE4kF3ES"
      },
      "source": [
        "### Métricas de desempeño\n",
        "\n",
        "Métricas basadas en contar datos correcta e incorrectamente clasificados:\n",
        "\n",
        "- **Accuracy (Exactitud)**: $$\\text{accuracy} = \\frac{\\text{número de predicciones correctas}}{\\text{número de predicciones totales}}$$\n",
        "\n",
        "- **Error rate (Tasa de error)**: $$\\text{error rate} = \\frac{\\text{número de predicciones incorrectas}}{\\text{número de predicciones totales}}$$\n",
        "\n",
        "\n",
        "\n",
        "- En nuestro ejemplo anterior: \n",
        "\n",
        "$$\\text{accuracy} = \\frac{9810}{10000} = 0.981$$\n",
        "\n",
        "$$\\text{error rate} = \\frac{190}{10000} = 0.019$$\n",
        "\n",
        "\n",
        "> **Pregunta ❓:** ¿Cuál es el problema de `Accuracy` en nuestro ejemplo?\n",
        "\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yg6KuykhF3ET"
      },
      "source": [
        "#### Métricas Basadas en la Matriz de Confusión\n",
        "\n",
        "Una posible solución a este problema son las métricas basadas en la matriz de confusión:\n",
        "\n",
        "<div align='center'>\n",
        "    <img src=\"https://i.ibb.co/5sMqPDR/matriz-conf.png\" alt=\"Ejemplo de una matriz de confusión para un problema de clasificación binario\" width=450/>\n",
        "</div>\n",
        "\n",
        "- **Precision**:  Fracción de ejemplos correctamente predichos como `+` con respecto a todos los predichos `+`.\n",
        "\n",
        "$$P = \\frac{\\text{Clasificados correctamente como positivo}}{\\text{Todos los predichos como positivos}} =\\frac{TP}{(TP + FP)}$$\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "- **Recall**: Fracción de ejemplos `+` que son correctamente clasificados: \n",
        "\n",
        "$$R = \\frac{\\text{Clasificados correctamente como positivo}}{\\text{Todos los que debería haber clasificado como positivos}}  = \\frac{TP}{(TP+FN)}$$\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "- **F1 measure**: Combina precisión y recall usando una media armónica (i.e., media que castiga si ambos valores son muy diferentes).\n",
        "\n",
        "$$F = \\frac{2PR}{(P+R)}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlLc3w9qF3ET"
      },
      "source": [
        "\n",
        "|                    | **Predicha (`+`)**  | **Predicha (`-`)** |\n",
        "|--------------------|---------------------|--------------------|\n",
        "| **Real (`+`)**     | 10                  | 90                 |\n",
        "| **Real (`-`)**     | 100                 | 9800               |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_G9a6Y3F3ET"
      },
      "source": [
        "En nuestro ejemplo anterior:\n",
        "\n",
        "\n",
        "$$P = \\frac{10}{110} = 0.\\bar{09}$$\n",
        "\n",
        "$$R = \\frac{10}{100} = 0.1$$\n",
        "\n",
        "$$F = \\frac{2 \\cdot 0.1 \\cdot 0.\\bar{1}}{(0.1 + 0.\\bar{1})} \\approx 0.095$$ \n",
        "\n",
        "Ahora claramente se nota el problema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "LbJOOg4zF3ET"
      },
      "outputs": [],
      "source": [
        "p = 10 / 110\n",
        "r = 10 / 100\n",
        "\n",
        "f = 2 * p * r / (p + r)\n",
        "f"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1oA0tVSF3ET"
      },
      "source": [
        "#### Matriz de confusión multiclase\n",
        "\n",
        "Cuando tenemos $k$ clases, la matriz de confusión es una matriz de $k \\times k$.\n",
        "\n",
        "<div align='center'>\n",
        "<img src=\"https://i.ibb.co/Z2Strv9/matriz-conf-multiclase.png\" alt=\"Ejemplo de una matriz de confusión para un problema de clasificación binario\" style=\"width: 500px;\"/>\n",
        "</div>\n",
        "\n",
        "¿Cómo calculamos las métricas?\n",
        "\n",
        "\n",
        "#### Métricas de Desempeño Generalizadas: \n",
        "\n",
        "\n",
        "- Precision: Fracción de ejemplos asignados a la clase `i` que son realmente de la clase `i`.\n",
        "\n",
        "$$\\text{precision} = \\frac{c_{ii}}{\\sum_{j}c_{ji}}$$\n",
        "\n",
        "\n",
        "- Recall: Fracción de ejemplos de la clase `i` correctamente clasificados: \n",
        "\n",
        "$$\\text{recall} = \\frac{c_{ii}}{\\sum_{j}c_{ij}}$$\n",
        "\n",
        "- Accuracy: Fracción de ejemplos correctamente clasificados:\n",
        "\n",
        "$$\\text{accuracy} = \\frac{\\sum_{i}c_{ii}}{\\sum_{j}\\sum_{i}c_{ij}}$$\n",
        "\n",
        "#### Estrategia de Agregación\n",
        "\n",
        "- **Macroaveraging**\n",
        "    - Computar métrica para cada clase y luego promediar. \n",
        "    - Sobrerepresentan clases minoritarias al tratar a todas por igual.\n",
        "\n",
        "- **Weighted**\n",
        "    - Computar métrica para cada clase y luego hace un promedio ponderado por el número de ejemplos de esa clase.\n",
        "    - Al ser ponderado por el número de casos, da más prioridad a las clases frecuentes.\n",
        "\n",
        "\n",
        "`Scikit` provee un acceso rápido a todas estas métricas a través de su función `sklearn.metrics.classification_report`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVhjw7jOF3ET"
      },
      "source": [
        "### Underfitting y Overfitting\n",
        "\n",
        "Errores de entrenamiento o **Underfitting**. \n",
        "- Malos resultados sobre los datos de entrenamiento\n",
        "- El clasificador no tiene capacidad de aprender el patrón.\n",
        "\n",
        "Errores de generalización o **Overfitting**. \n",
        "- Malos resultados sobre datos nuevos \n",
        "- El modelo se hace demasiado específico a los datos de entrenamiento. \n",
        "\n",
        "\n",
        "<div align='center'>\n",
        "<img src='https://i.ibb.co/Sc7SBs2/tipos-fit.png' width=800/>\n",
        "</div>\n",
        "\n",
        "<div align='center'>\n",
        "    Fuente: The Hundred-Page Machine Learning Book.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-07T19:45:34.649513Z",
          "start_time": "2020-09-07T19:45:34.617566Z"
        },
        "id": "OUgg-zZtF3ET"
      },
      "source": [
        "## Nuestro problema de hoy: Pingüinos  🐧\n",
        "\n",
        "\n",
        "Origen del dataset:\n",
        "\n",
        "**Palmer Archipelago (Antarctica) penguin data**: \n",
        "\n",
        "*Data were collected and made available by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network.*\n",
        "\n",
        "https://github.com/allisonhorst/palmerpenguins\n",
        "\n",
        "![Pinguinos](https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/man/figures/lter_penguins.png)\n",
        "\n",
        "\n",
        "    \n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvjH5nNKF3EU"
      },
      "source": [
        "### Atributos\n",
        " \n",
        "- `culmen_length_mm`: Largo del culmen (vértice o borde superior de la mandíbula)  (mm).\n",
        "- `culmen_depth_mm`: Alto del culmen (vértice o borde superior de la mandíbula) (mm).\n",
        "- `flipper_length_mm`: Longitud de las aletas (mm).\n",
        "- `body_mass_g`: Masa corporal (g).\n",
        "- `island`: Isla de origen (Dream, Torgersen, or Biscoe) en el archipiélago de Palmer (Antarctica).\n",
        "- `sex`: Sexo del pinguino.\n",
        "\n",
        "![Detalle Variables](https://allisonhorst.github.io/palmerpenguins/reference/figures/culmen_depth.png)\n",
        "    \n",
        "<center>Créditos a Allison Horst por sus excelentes ilustraciones https://github.com/allisonhorst </center>    \n",
        "    \n",
        "    \n",
        "### Variable a predecir\n",
        "\n",
        "- `species`: Especie del pinguino (Chinstrap, Adélie, or Gentoo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-07T18:13:05.372393Z",
          "start_time": "2020-09-07T18:13:05.369428Z"
        },
        "id": "FB7wKYUEF3EU"
      },
      "source": [
        "## Exploración y Preprocesamiento\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T17:43:11.776288Z",
          "start_time": "2020-09-11T17:43:00.371984Z"
        },
        "scrolled": true,
        "id": "8CWaSbP8F3EU"
      },
      "outputs": [],
      "source": [
        "# Instalar graphviz para visualizar el árbol generado\n",
        "# Deben instalar antes graphviz: https://www.graphviz.org/download/\n",
        "\n",
        "import sys\n",
        "\n",
        "!pip install graphviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T17:43:11.786261Z",
          "start_time": "2020-09-11T17:43:11.778282Z"
        },
        "id": "_HD7R_ETF3EU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T17:43:11.818175Z",
          "start_time": "2020-09-11T17:43:11.788255Z"
        },
        "id": "yM46LZwnF3EU"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"penguins.csv\").dropna().reset_index(drop=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T17:43:11.906937Z",
          "start_time": "2020-09-11T17:43:11.822164Z"
        },
        "id": "hlq2y714F3EU"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "fig = px.scatter_matrix(\n",
        "    df, dimensions=df.iloc[:, 2:].columns, color=\"species\", height=1000\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3YHwJsNF3EU"
      },
      "outputs": [],
      "source": [
        "px.parallel_categories(df, dimensions=[\"island\", \"species\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uv0A22bF3EU"
      },
      "outputs": [],
      "source": [
        "df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWRKeyFVF3EU"
      },
      "source": [
        "---\n",
        "\n",
        "## Holdout\n",
        "\n",
        "Consiste en particionar nuestro dataset en conjuntos de:\n",
        "\n",
        "- **Training**: conjunto que se utiliza para **entrenar** el modelo.\n",
        "- **Testing**: datos que se usa para **evaluar** qué tan bien predice el modelo (a través de las métricas de evaluación). \n",
        "\n",
        "\n",
        "Comunmente se dividen en proporción $2/3$ y $1/3$ del dataset respectivamente. Sin embargo, todo depende de la cantidad de datos que se posean: si se tiene millones de ejemplos, quizas puede dividirse en 95% train, 5% test sin problemas. \n",
        "\n",
        "\n",
        "La evaluación puede variar mucho según las particiones escogidas: \n",
        "\n",
        "- Training pequeño -> modelo sesgado, \n",
        "- Testing pequeño -> evaluación poco confiable.\n",
        "\n",
        "\n",
        "Esta ténica se puede **Random Subsampling** para seleccionar aleatoriamente las observaciones de cada uno de estos conjuntos.\n",
        "\n",
        "Para ejecutar todo esto usaremos `train_test_split`. Veamos algunos de sus parámetros:\n",
        "\n",
        "- `test_size = 0.33` - indica el tamaño del test de evaluación.\n",
        "- `shuffle = True` - indica que ejecutaremos Random Subsampling.\n",
        "- `stratify = labels` - intenta manetener la distribución de clases original en ambos conjuntos.\n",
        "\n",
        "\n",
        "### Validation set:\n",
        "\n",
        "Cuando se desea realizar una búsqueda de los mejores algoritmos y sus hiperparámetros, el dataset puede ser dividido en 3:\n",
        "\n",
        "\n",
        "- **Training**: Se utiliza para entrenar los modelos.\n",
        "- **Validation**: Se utiliza para seleccionar el mejor modelo al ir variando sus hiperparámetros.\n",
        "- **Testing**: Se utiliza para evaluar el modelo previo a ser entregado o puesto en producción. Esta evaluación solo se hace sobre el modelo final.\n",
        "\n",
        "\n",
        "En este caso la división puede ser $70\\%, 15\\%, 15\\%$ respectivamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T17:43:12.091444Z",
          "start_time": "2020-09-11T17:43:12.083466Z"
        },
        "id": "JnfNCKAfF3EU"
      },
      "outputs": [],
      "source": [
        "# Holdout\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "features = df.drop(columns=[\"species\"])\n",
        "labels = df.loc[:, \"species\"]\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    features, labels, test_size=0.33, shuffle=True, stratify=labels\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_ByrBfhF3EU"
      },
      "outputs": [],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T17:43:12.118372Z",
          "start_time": "2020-09-11T17:43:12.094436Z"
        },
        "scrolled": true,
        "id": "-R_QwYUqF3EU"
      },
      "outputs": [],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fWlakDPF3EV"
      },
      "outputs": [],
      "source": [
        "y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxIJPDBUF3EV"
      },
      "outputs": [],
      "source": [
        "# distribución original\n",
        "labels.value_counts() / labels.count() * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "_BMNvG80F3EV"
      },
      "outputs": [],
      "source": [
        "# conjunto de entrenamiento\n",
        "y_train.value_counts() / y_train.count() * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "UZlG2KJjF3EV"
      },
      "outputs": [],
      "source": [
        "# conjunto de pruebas\n",
        "y_test.value_counts() / y_test.count() * 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtT4CfEJF3EV"
      },
      "source": [
        "\n",
        "\n",
        "### Otra Opción: `cross-validation`\n",
        "\n",
        "Se particiona el dataset en k conjuntos disjuntos o folds:\n",
        "\n",
        "---\n",
        "    Para cada partición i:\n",
        "        - Juntar todas las k-1 particiones restantes y entrenar el modelo sobre esos datos.\n",
        "        - Evaluar el modelo en la partición i.\n",
        "        \n",
        "    El error total = suma de errores de todos los modelos  \n",
        "\n",
        "---\n",
        "\n",
        "<img src='https://i.ibb.co/Sc7SBs2/tipos-fit.png' width=400>\n",
        "\n",
        "\n",
        "Veremos más de esto en las próximas clases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-tQDffNF3EV"
      },
      "source": [
        "### Preprocesamiento y Data Leakage\n",
        "\n",
        "\n",
        "Data Leakage o fuga de datos se refiere al uso de datos de prueba dentro del entrenamiento de un modelo predictivo (lo que ovbiamente es incorrecto).\n",
        "\n",
        "Es muy importante que el **preprocesamiento y feature engineering lo hagan siempre sobre los datos de entrenamiento y no sobre todo el dataset**. De lo contrario, estarían ocupando datos destinados a evaluar para entrenar el modelo (o el preprocesamiento) lo que puede inducir a resultados muy buenos cuando en verdad no deberían serlos.\n",
        "\n",
        "\n",
        "Mas información en [data-leakage de scikit-learn](https://scikit-learn.org/stable/common_pitfalls.html#data-leakage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvuy-OtkF3EV"
      },
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, RobustScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21qV7sq0F3EV"
      },
      "outputs": [],
      "source": [
        "ct = ColumnTransformer(\n",
        "    [\n",
        "        (\n",
        "            \"Scaler\",\n",
        "            RobustScaler(),\n",
        "            [\n",
        "                \"culmen_length_mm\",\n",
        "                \"culmen_depth_mm\",\n",
        "                \"flipper_length_mm\",\n",
        "                \"body_mass_g\",\n",
        "            ],\n",
        "        ),\n",
        "        (\"OneHot\", OneHotEncoder(sparse=False), [\"island\", \"sex\"]),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2Rg3C41F3EV"
      },
      "outputs": [],
      "source": [
        "X_train_preprocessed = pd.DataFrame(\n",
        "    ct.fit_transform(X_train),\n",
        "    columns=np.concatenate(\n",
        "        [ct.transformers_[0][2], ct.transformers_[1][1].get_feature_names()], axis=0\n",
        "    ),\n",
        ")\n",
        "\n",
        "X_train_preprocessed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "gZDz9DSfF3EV"
      },
      "source": [
        "---\n",
        "\n",
        "## Árboles de Decisión\n",
        "\n",
        "Árboles que fragmentan el dataset en condiciones.\n",
        "\n",
        "- Nodo raíz: Sin arcos entrantes, 2 o más salientes. Contienen una condición sobre alguna feature.\n",
        "- Nodo interno: 1 arco entrante, 2 o más salientes.  Contienen una condición sobre alguna feature.\n",
        "- Nodo hoja/terminal: 1 arco entrante, nunguno saliente. Indican la clase.\n",
        "\n",
        "\n",
        "Nota: `tree` de `Scikit-learn` solo implementa 2 ramas salientes en los nodos raíz y interno.\n",
        "\n",
        "Se entrenan recursivamente:\n",
        "\n",
        "Estrategia: Top down (greedy) - Divide y vencerás:\n",
        "\n",
        "---\n",
        "    Seleccionar un atributo para el nodo raíz y crear rama para cada valor posible del atributo.\n",
        "    Luego: dividir las instancias del dataset en subconjuntos, uno para cada rama que se extiende desde el nodo.\n",
        "    Por último: repetir de forma recursiva para cada rama, utilizando sólo las instancias que llegan a ésta.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T17:43:12.133332Z",
          "start_time": "2020-09-11T17:43:12.121364Z"
        },
        "id": "xvtfekg2F3EV"
      },
      "outputs": [],
      "source": [
        "import graphviz\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "\n",
        "tree_pipe = Pipeline(\n",
        "    [(\"preprocesamiento\", ct), (\"tree\", DecisionTreeClassifier(criterion=\"entropy\"))]\n",
        ")\n",
        "\n",
        "# noten que aquí se pasa X_train ya que la etapa de\n",
        "# preprocesamiento está incluida en el pipeline (primera etapa)\n",
        "\n",
        "tree_pipe = tree_pipe.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T17:43:12.153278Z",
          "start_time": "2020-09-11T17:43:12.135327Z"
        },
        "id": "cGIlq9i6F3EV"
      },
      "outputs": [],
      "source": [
        "y_pred = tree_pipe.predict(X_test)\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2KUsQkOF3EW"
      },
      "source": [
        "### Evaluación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNaUe2X8F3EW"
      },
      "outputs": [],
      "source": [
        "print(\"Matriz de confusión\\n\\n\", confusion_matrix(y_test, y_pred), \"\\n\")\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOOUxRGyF3EW"
      },
      "source": [
        "### Visualizar el árbol"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSbivGeYF3EW"
      },
      "outputs": [],
      "source": [
        "# con esto obtenemos los nombres de las columnas\n",
        "# en el primer elemento del arreglo de obtienen las numéricas a partir\n",
        "# de la lista de la primera transformación y en la segunda se obtienen a partir\n",
        "# de las columnas generadas por el one hot encoding.\n",
        "cols_names = np.concatenate(\n",
        "    [\n",
        "        tree_pipe.steps[0][1].transformers_[0][2],\n",
        "        tree_pipe.steps[0][1].transformers_[1][1].get_feature_names(),\n",
        "    ]\n",
        ")\n",
        "cols_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLbZVnRtF3EW"
      },
      "source": [
        "Para ejecutar la siguiente celda necesitarán tener instalado Graphviz:\n",
        "\n",
        "https://graphviz.org/download/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T17:43:12.239049Z",
          "start_time": "2020-09-11T17:43:12.156271Z"
        },
        "id": "NA3bww2xF3EW"
      },
      "outputs": [],
      "source": [
        "dot_data = export_graphviz(\n",
        "    tree_pipe.steps[1][1],\n",
        "    out_file=None,\n",
        "    feature_names=cols_names,\n",
        "    class_names=labels.unique(),\n",
        "    filled=True,\n",
        "    rounded=True,\n",
        "    special_characters=True,\n",
        ")\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s4q6FQnF3EW"
      },
      "source": [
        "> **Pregunta ❓**: ¿Cómo eligo los atributos y sus divisiones? \n",
        "\n",
        "- La idea es ir dividiendo el dataset en nodos a la vez que se crea el árbol más pequeño posible.\n",
        "- Heurística: escoge el atributo cuya división que produce nodos lo más “puros” posibles (es decir, que pertenezcan mayoritariamente a la misma clase). El criterio comunmente utilizado es Information Gain.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI6l4hclF3EW"
      },
      "source": [
        "### Resumen Árboles de decisión\n",
        "\n",
        "\n",
        "\n",
        "| Ventajas                                | Deseventajas                                                                   |\n",
        "|-----------------------------------------|--------------------------------------------------------------------------------|\n",
        "| Simple de entender y interpretar.       | Se deben preprocesar las variables categóricas y ordinales antes de ser usadas |\n",
        "| Pueden ser visualizados.                | No escala tan bien a muchas decisiones (crece mucho el árbol)                  |\n",
        "| Al ser árbol, es muy rápido de evaluar. |                                                                                |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTE1sPTYF3EW"
      },
      "source": [
        "## Extra: Guía de Scikit-learn para Elegir el Modelo\n",
        "\n",
        "<img src='https://i.ibb.co/1XkHvRC/ml-map.png' />"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "yg6KuykhF3ET",
        "q1oA0tVSF3ET",
        "FRNAspneF3EW"
      ]
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}